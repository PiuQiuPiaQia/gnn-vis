#!/usr/bin/env python
# coding: utf-8

# # ä»é›¶å¼€å§‹è¿è¡Œ GraphCast ï¼ˆAutoDL æˆ–è€…å…¶ä»–æ–°çš„ç¯å¢ƒï¼‰
# -------------------------------------------------------------------
# **è¿™æ˜¯ä» https://google-deepmind/graphcast å¤ç°çš„é¡¹ç›®ã€‚ç”± https://github.com/sfsun67 æ”¹å†™å’Œè°ƒè¯•ã€‚**
# 
# **AutoDL æ˜¯å›½å†…çš„ä¸€å®¶äº‘è®¡ç®—å¹³å°ï¼Œç½‘å€æ˜¯https://www.autodl.com**
# 
# ä½ åº”è¯¥æœ‰ç±»ä¼¼çš„æ–‡ä»¶ç»“æ„ï¼Œè¿™é‡Œçš„æ•°æ®ç”± Google Cloud Bucket (https://console.cloud.google.com/storage/browser/dm_graphcast æä¾›ã€‚æ¨¡å‹æƒé‡ã€æ ‡å‡†åŒ–ç»Ÿè®¡å’Œç¤ºä¾‹è¾“å…¥å¯åœ¨Google Cloud Bucketä¸Šæ‰¾åˆ°ã€‚å®Œæ•´çš„æ¨¡å‹è®­ç»ƒéœ€è¦ä¸‹è½½ERA5æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¯ä»ECMWFè·å¾—ã€‚
# ```
# .
# â”œâ”€â”€ code
# â”‚   â”œâ”€â”€ GraphCast-from-Ground-Zero
# â”‚       â”œâ”€â”€graphcast
# â”‚       â”œâ”€â”€tree
# â”‚       â”œâ”€â”€wrapt
# â”‚       â”œâ”€â”€graphcast_demo.ipynb
# â”‚       â”œâ”€â”€README.md
# â”‚       â”œâ”€â”€setup.py
# â”‚       â”œâ”€â”€...
# â”œâ”€â”€ data
# â”‚   â”œâ”€â”€ dataset
# â”‚       â”œâ”€â”€dataset-source-era5_date-2022-01-01_res-1.0_levels-13_steps-01.nc
# â”‚       â”œâ”€â”€dataset-source-era5_date-2022-01-01_res-1.0_levels-13_steps-04.nc
# â”‚       â”œâ”€â”€dataset-source-era5_date-2022-01-01_res-1.0_levels-13_steps-12.nc
# â”‚       â”œâ”€â”€...
# â”‚   â”œâ”€â”€ params
# â”‚       â”œâ”€â”€params-GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz
# â”‚       â”œâ”€â”€params-GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz
# â”‚       â”œâ”€â”€...
# â”‚   â”œâ”€â”€ stats
# â”‚       â”œâ”€â”€stats-mean_by_level.nc
# â”‚       â”œâ”€â”€...
# â””â”€â”€â”€â”€â”€â”€ 
# ```
# 
# PS: 
# 1. Python è¦ä½¿ç”¨3.10ç‰ˆæœ¬ã€‚è€ç‰ˆæœ¬ä¼šå‡ºç°å‡½æ•°è°ƒç”¨å¤±æ•ˆçš„é—®é¢˜ã€‚
# 2. ä½ éœ€è¦ä»”ç»†æ ¸å¯¹åŒ…çš„ç‰ˆæœ¬ï¼Œé˜²æ­¢å‡ºç°æ„å¤–çš„é”™è¯¯ã€‚ä¾‹å¦‚ï¼Œ xarray åªèƒ½ä½¿ç”¨ 2023.7.0 ç‰ˆæœ¬ï¼Œå…¶ä»–ç‰ˆæœ¬ä¼šå‡ºç°é”™è¯¯ã€‚
# 3. ä½ éœ€è¦ä»”ç»†æ ¸å¯¹æ‰€æœ‰åŒ…æ˜¯å¦å®‰è£…æ­£ç¡®ã€‚æœªå®‰è£…çš„åŒ…ä¼šå¯¼è‡´æ„å¤–é”™è¯¯ã€‚ä¾‹å¦‚ï¼Œtree å’Œ wrapt æ˜¯ä¸¤ä¸ª GraphCast æ‰€å¿…éœ€çš„åŒ…ï¼Œä½†æ˜¯å¹¶ä¸åœ¨æºæ–‡ä»¶ä¸­ã€‚ä¾‹å¦‚ï¼Œtree å’Œ wrapt ä¸­çš„ .os æ–‡ä»¶æœªå¯¼å…¥ï¼Œä¼šå¼•å‘å¾ªç¯è°ƒç”¨ã€‚ä»–ä»¬çš„åŸå§‹æ–‡ä»¶å¯ä»¥åœ¨ Colaboratory(https://colab.research.google.com/github/deepmind/graphcast/blob/master/graphcast_demo.ipynb) çš„ç¯å¢ƒä¸­æ‰¾åˆ°ã€‚
# 
# 
# 
# *ä»£ç åœ¨å¦‚ä¸‹æœºå™¨ä¸Šæµ‹è¯•*
# 1. GPU: TITAN Xp 12GB; CPU: Xeon(R) E5-2680 v4;  JAX / 0.3.10 / 3.8(ubuntu18.04) / 11.1
# 2. GPU: V100-SXM2-32GB 32GB; CPU: Xeon(R) Platinum 8255C; JAX / 0.3.10 / 3.8(ubuntu18.04) / 11.1
# 3. GPU: RTX 2080 Ti(11GB); CPU: Xeon(R) Platinum 8255C; JAX / 0.3.10 / 3.8(ubuntu18.04) / 11.1
# -------------------------------------------------------------------
# 

# <p><small><small>ç‰ˆæƒæ‰€æœ‰ 2023 å¹´ DeepMind Technologies Limitedã€‚</small></small></p>
# <p><small><small>æ ¹æ® Apache è®¸å¯è¯ç¬¬ 2.0 ç‰ˆï¼ˆ"è®¸å¯è¯"ï¼‰è·å¾—è®¸å¯ï¼›é™¤éç¬¦åˆè®¸å¯è¯çš„è§„å®šï¼Œå¦åˆ™æ‚¨ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a> è·å–è®¸å¯è¯çš„å‰¯æœ¬ã€‚</small></small></p>
# <p><small><small>é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯åŸºäº "æŒ‰åŸæ ·" åˆ†å‘çš„ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚æœ‰å…³è®¸å¯è¯ä¸‹çš„å…·ä½“è¯­è¨€ï¼Œè¯·å‚è§è®¸å¯è¯ä¸­çš„æƒé™å’Œé™åˆ¶ã€‚</small></small></p>
# 

# # å°† Python ç‰ˆæœ¬æ›´æ–°åˆ° 3.10.
# 
# GraphCast éœ€è¦ Python >= 3.10 ã€‚æ¨è Python 3.10ã€‚
# 
# åœ¨ç»ˆç«¯ä¸­ï¼Œæ–°å»ºä¸€ä¸ªåä¸º GraphCast çš„ç¯å¢ƒã€‚
# 
# å‚è€ƒä»£ç å¦‚ä¸‹ï¼š
# ```
# 
# # æ›´æ–° conda ï¼ˆå¯é€‰ï¼‰
# conda update -n base -c defaults conda
# 
# # åœ¨æ–°ç¯å¢ƒ GraphCast ä¸­å®‰è£… python=3.10  
# conda create -n GraphCast python=3.10    
# 
# # æ›´æ–°bashrcä¸­çš„ç¯å¢ƒå˜é‡
# conda init bash && source /root/.bashrc
# 
# # æ¿€æ´»æ–°çš„ç¯å¢ƒ
# conda activate GraphCast
# 
# # éªŒè¯ç‰ˆæœ¬
# python --version
# 
# # åœ¨ Jupyter ä¸­æ³¨å†Œ Python 3.10 ç¯å¢ƒ
# # å®‰è£… ipykernel åŒ…
# conda install ipykernel
# 
# # æ³¨å†Œçš„ Python 3.10 ç¯å¢ƒçš„å†…æ ¸åç§°
# python -m ipykernel install --user --name=GraphCast-python3.10
# ```
# 
# æ³¨æ„ï¼šJupyter æ³¨å†Œ Python 3.10 ç¯å¢ƒåï¼Œé‡å¯jupyterï¼Œä½¿ç”¨æ–°çš„å†…æ ¸ GraphCast-python3.10ã€‚

# # å®‰è£…å’Œåˆå§‹åŒ–
# 

# In[3]:


from jax.lib import xla_bridge
import jax
print("JAX devices:", jax.devices())


# In[4]:


# å­¦æœ¯èµ„æºåŠ é€Ÿ https://www.autodl.com/docs/network_turbo/  .

# import subprocess
# import os

# result = subprocess.run('bash -c "source /etc/network_turbo && env | grep proxy"', shell=True, capture_output=True, text=True)
# output = result.stdout
# for line in output.splitlines():
#     if '=' in line:
#         var, value = line.split('=', 1)
#         os.environ[var] = value


# In[5]:


# è¿™ä¸€æ­¥å°†ä½¿ç”¨ shapely å®‰è£…ç¯å¢ƒã€‚ä¸ºäº†é¿å…å‡ºç°ERRORï¼š æ— æ³•ä¸º shapely æ„å»ºè½®å­ï¼Œè€Œå®‰è£…åŸºäº pyproject.toml çš„é¡¹ç›®éœ€è¦è½®å­ã€‚

# !pip uninstall -y shapely
# !conda install -y shapely
# !pip uninstall -y shapely


# In[6]:


# @title Pip å®‰è£… graphcast å’Œå…¶ä»–ä¾èµ–é¡¹


# %pip install --upgrade https://github.com/deepmind/graphcast/archive/master.zip


# In[7]:


# @title cartopy å´©æºƒçš„è§£å†³æ–¹æ³•

# !pip uninstall -y shapely
# !pip install shapely --no-binary shapely


# In[8]:


# @title å®‰è£…å…¶ä»–ä¾èµ–é¡¹ï¼Œå¹¶è§£å†³ xarray çš„ç‰ˆæœ¬é—®é¢˜ã€‚

# è¿™é‡Œéœ€è¦å°†xarrayçš„ç‰ˆæœ¬ä»2023.12.0(2023å¹´12æœˆ30æ—¥å®‰è£…)é™ä½åˆ°2023.7.0ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚

# !conda install -y -c conda-forge ipywidgets
# !pip uninstall -y xarray
# !pip install xarray==2023.7.0


# In[9]:


# @title å¯¼å…¥åº“


import dataclasses
import datetime
import functools
import math
import re
from typing import Optional

import cartopy.crs as ccrs
#from google.cloud import storage
from graphcast import autoregressive
from graphcast import casting
from graphcast import checkpoint
from graphcast import data_utils
from graphcast import graphcast
from graphcast import normalization
from graphcast import rollout
from graphcast import xarray_jax
from graphcast import xarray_tree
from IPython.display import HTML
import ipywidgets as widgets
import haiku as hk
import jax
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import animation
import numpy as np
import xarray




def parse_file_parts(file_name):
  return dict(part.split("-", 1) for part in file_name.split("_"))


# In[10]:


# @title è½½å…¥ç»˜å›¾å‡½æ•°


def select(
    data: xarray.Dataset,
    variable: str,
    level: Optional[int] = None,
    max_steps: Optional[int] = None
    ) -> xarray.Dataset:
  data = data[variable]
  if "batch" in data.dims:
    data = data.isel(batch=0)
  if max_steps is not None and "time" in data.sizes and max_steps < data.sizes["time"]:
    data = data.isel(time=range(0, max_steps))
  if level is not None and "level" in data.coords:
    data = data.sel(level=level)
  return data

def scale(
    data: xarray.Dataset,
    center: Optional[float] = None,
    robust: bool = False,
    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:
  vmin = np.nanpercentile(data, (2 if robust else 0))
  vmax = np.nanpercentile(data, (98 if robust else 100))
  if center is not None:
    diff = max(vmax - center, center - vmin)
    vmin = center - diff
    vmax = center + diff
  return (data, matplotlib.colors.Normalize(vmin, vmax),
          ("RdBu_r" if center is not None else "viridis"))

def plot_data(
    data: dict[str, xarray.Dataset],
    fig_title: str,
    plot_size: float = 5,
    robust: bool = False,
    cols: int = 4
    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:

  first_data = next(iter(data.values()))[0]
  max_steps = first_data.sizes.get("time", 1)
  assert all(max_steps == d.sizes.get("time", 1) for d, _, _ in data.values())

  cols = min(cols, len(data))
  rows = math.ceil(len(data) / cols)
  figure = plt.figure(figsize=(plot_size * 2 * cols,
                               plot_size * rows))
  figure.suptitle(fig_title, fontsize=16)
  figure.subplots_adjust(wspace=0, hspace=0)
  figure.tight_layout()

  images = []
  for i, (title, (plot_data, norm, cmap)) in enumerate(data.items()):
    ax = figure.add_subplot(rows, cols, i+1)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(title)
    im = ax.imshow(
        plot_data.isel(time=0, missing_dims="ignore"), norm=norm,
        origin="lower", cmap=cmap)
    plt.colorbar(
        mappable=im,
        ax=ax,
        orientation="vertical",
        pad=0.02,
        aspect=16,
        shrink=0.75,
        cmap=cmap,
        extend=("both" if robust else "neither"))
    images.append(im)

  def update(frame):
    if "time" in first_data.dims:
      td = datetime.timedelta(microseconds=first_data["time"][frame].item() / 1000)
      figure.suptitle(f"{fig_title}, {td}", fontsize=16)
    else:
      figure.suptitle(fig_title, fontsize=16)
    for im, (plot_data, norm, cmap) in zip(images, data.values()):
      im.set_data(plot_data.isel(time=frame, missing_dims="ignore"))

  ani = animation.FuncAnimation(
      fig=figure, func=update, frames=max_steps, interval=250)
  plt.close(figure.number)
  return HTML(ani.to_jshtml())


# # åŠ è½½æ•°æ®å¹¶åˆå§‹åŒ–æ¨¡å‹

# ## è½½å…¥æ¨¡å‹å‚æ•°
# 
# é€‰æ‹©ä¸¤ç§è·å–æ¨¡å‹å‚æ•°çš„æ–¹å¼ä¹‹ä¸€ï¼š
# - **random**ï¼šæ‚¨å°†è·å¾—éšæœºé¢„æµ‹ï¼Œä½†æ‚¨å¯ä»¥æ›´æ”¹æ¨¡å‹æ¶æ„ï¼Œè¿™å¯èƒ½ä¼šä½¿å…¶è¿è¡Œæ›´å¿«æˆ–é€‚åº”æ‚¨çš„è®¾å¤‡ã€‚
# - **checkpoint**ï¼šæ‚¨å°†è·å¾—æ˜æ™ºçš„é¢„æµ‹ï¼Œä½†å—é™äºæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¶æ„ï¼Œè¿™å¯èƒ½ä¸é€‚åˆæ‚¨çš„è®¾å¤‡ã€‚ç‰¹åˆ«æ˜¯ç”Ÿæˆæ¢¯åº¦ä¼šä½¿ç”¨å¤§é‡å†…å­˜ï¼Œå› æ­¤æ‚¨è‡³å°‘éœ€è¦25GBçš„å†…å­˜ï¼ˆTPUv4æˆ–A100ï¼‰ã€‚
# 
# æ£€æŸ¥ç‚¹åœ¨ä¸€äº›æ–¹é¢æœ‰æ‰€ä¸åŒï¼š
# - ç½‘æ ¼å¤§å°æŒ‡å®šäº†åœ°çƒçš„å†…éƒ¨å›¾å½¢è¡¨ç¤ºã€‚è¾ƒå°çš„ç½‘æ ¼å°†è¿è¡Œæ›´å¿«ï¼Œä½†è¾“å‡ºå°†æ›´å·®ã€‚ç½‘æ ¼å¤§å°ä¸å½±å“æ¨¡å‹çš„å‚æ•°æ•°é‡ã€‚
# - åˆ†è¾¨ç‡å’Œå‹åŠ›çº§åˆ«çš„æ•°é‡å¿…é¡»åŒ¹é…æ•°æ®ã€‚è¾ƒä½çš„åˆ†è¾¨ç‡å’Œè¾ƒå°‘çš„çº§åˆ«ä¼šè¿è¡Œå¾—æ›´å¿«ã€‚æ•°æ®åˆ†è¾¨ç‡ä»…å½±å“ç¼–ç å™¨/è§£ç å™¨ã€‚
# - æˆ‘ä»¬çš„æ‰€æœ‰æ¨¡å‹éƒ½é¢„æµ‹é™æ°´ã€‚ç„¶è€Œï¼ŒERA5åŒ…å«é™æ°´ï¼Œè€ŒHRESä¸åŒ…å«ã€‚æˆ‘ä»¬æ ‡è®°ä¸º "ERA5" çš„æ¨¡å‹å°†é™æ°´ä½œä¸ºè¾“å…¥ï¼Œå¹¶æœŸæœ›ä»¥ERA5æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œè€Œæ ‡è®°ä¸º "ERA5-HRES" çš„æ¨¡å‹ä¸ä»¥é™æ°´ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸“é—¨è®­ç»ƒä»¥HRES-fc0ä½œä¸ºè¾“å…¥ï¼ˆè¯·å‚é˜…ä¸‹é¢çš„æ•°æ®éƒ¨åˆ†ï¼‰ã€‚
# 
# æˆ‘ä»¬æä¾›ä¸‰ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼š
# 1. `GraphCast`ï¼Œç”¨äºGraphCastè®ºæ–‡çš„é«˜åˆ†è¾¨ç‡æ¨¡å‹ï¼ˆ0.25åº¦åˆ†è¾¨ç‡ï¼Œ37ä¸ªå‹åŠ›çº§åˆ«ï¼‰ï¼Œåœ¨1979å¹´è‡³2017å¹´é—´ä½¿ç”¨ERA5æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œ
# 
# 2. `GraphCast_small`ï¼ŒGraphCastçš„è¾ƒå°ä½åˆ†è¾¨ç‡ç‰ˆæœ¬ï¼ˆ1åº¦åˆ†è¾¨ç‡ï¼Œ13ä¸ªå‹åŠ›çº§åˆ«å’Œè¾ƒå°çš„ç½‘æ ¼ï¼‰ï¼Œåœ¨1979å¹´è‡³2015å¹´é—´ä½¿ç”¨ERA5æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œé€‚ç”¨äºå…·æœ‰è¾ƒä½å†…å­˜å’Œè®¡ç®—çº¦æŸçš„æ¨¡å‹è¿è¡Œï¼Œ
# 
# 3. `GraphCast_operational`ï¼Œä¸€ä¸ªé«˜åˆ†è¾¨ç‡æ¨¡å‹ï¼ˆ0.25åº¦åˆ†è¾¨ç‡ï¼Œ13ä¸ªå‹åŠ›çº§åˆ«ï¼‰ï¼Œåœ¨1979å¹´è‡³2017å¹´ä½¿ç”¨ERA5æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨2016å¹´è‡³2021å¹´é—´ä½¿ç”¨HRESæ•°æ®è¿›è¡Œå¾®è°ƒã€‚æ­¤æ¨¡å‹å¯ä»¥ä»HRESæ•°æ®åˆå§‹åŒ–ï¼ˆä¸éœ€è¦é™æ°´è¾“å…¥ï¼‰ã€‚
# 

# In[11]:


# @title é€‰æ‹©æ¨¡å‹
# Rewrite by S.F. Sune, https://github.com/sfsun67.
'''
    æˆ‘ä»¬æœ‰ä¸‰ç§è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä¾›é€‰æ‹©, éœ€è¦ä»https://console.cloud.google.com/storage/browser/dm_graphcastå‡†å¤‡ï¼š
    GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz
    GraphCast_operational - ERA5-HRES 1979-2021 - resolution 0.25 - pressure levels 13 - mesh 2to6 - precipitation output only.npz
    GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz
'''
# åœ¨æ­¤è·¯å¾„ /root/data/params ä¸­æŸ¥æ‰¾ç»“æœï¼Œå¹¶åˆ—å‡º "params/"ä¸­æ‰€æœ‰æ–‡ä»¶çš„åç§°ï¼Œå»æ‰åç§°ä¸­çš„ "params/"perfixã€‚

import os
import glob

# å®šä¹‰æ•°æ®ç›®å½•ï¼Œè¯·æ›¿æ¢æˆä½ è‡ªå·±çš„ç›®å½•ã€‚
dir_path_params = "/root/data/params"

# Use glob to get all file paths in the directory
file_paths_params = glob.glob(os.path.join(dir_path_params, "*"))

# Remove the directory path and the ".../params/" prefix from each file name
params_file_options = [os.path.basename(path) for path in file_paths_params]


random_mesh_size = widgets.IntSlider(
    value=4, min=4, max=6, description="Mesh size:")
random_gnn_msg_steps = widgets.IntSlider(
    value=4, min=1, max=32, description="GNN message steps:")
random_latent_size = widgets.Dropdown(
    options=[int(2**i) for i in range(4, 10)], value=32,description="Latent size:")
random_levels = widgets.Dropdown(
    options=[13, 37], value=13, description="Pressure levels:")


params_file = widgets.Dropdown(
    options=params_file_options,
    description="Params file:",
    layout={"width": "max-content"})

source_tab = widgets.Tab([
    widgets.VBox([
        random_mesh_size,
        random_gnn_msg_steps,
        random_latent_size,
        random_levels,
    ]),
    params_file,
])
# è®¾ç½®é»˜è®¤æ˜¾ç¤ºçš„æ ‡ç­¾ç´¢å¼•
source_tab.selected_index = 1

source_tab.set_title(0, "éšæœºå‚æ•°æƒé‡ï¼ˆRandomï¼‰")
source_tab.set_title(1, "é¢„è®­ç»ƒæƒé‡ï¼ˆCheckpointï¼‰")
widgets.VBox([
    source_tab,
    widgets.Label(value="è¿è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼ä»¥åŠ è½½æ¨¡å‹ã€‚é‡æ–°è¿è¡Œè¯¥å•å…ƒæ ¼å°†æ¸…é™¤æ‚¨çš„é€‰æ‹©ã€‚")
])


# In[12]:


# @title åŠ è½½æ¨¡å‹

source = source_tab.get_title(source_tab.selected_index)

if source == "éšæœºå‚æ•°æƒé‡ï¼ˆRandomï¼‰":
  params = None  # Filled in below
  state = {}
  model_config = graphcast.ModelConfig(
      resolution=0,
      mesh_size=random_mesh_size.value,
      latent_size=random_latent_size.value,
      gnn_msg_steps=random_gnn_msg_steps.value,
      hidden_layers=1,
      radius_query_fraction_edge_length=0.6)
  task_config = graphcast.TaskConfig(
      input_variables=graphcast.TASK.input_variables,
      target_variables=graphcast.TASK.target_variables,
      forcing_variables=graphcast.TASK.forcing_variables,
      pressure_levels=graphcast.PRESSURE_LEVELS[random_levels.value],
      input_duration=graphcast.TASK.input_duration,
  )
else:
  assert source == "é¢„è®­ç»ƒæƒé‡ï¼ˆCheckpointï¼‰"
  '''with gcs_bucket.blob(f"params/{params_file.value}").open("rb") as f:
    ckpt = checkpoint.load(f, graphcast.CheckPoint)'''

  with open(f"{dir_path_params}/{params_file.value}", "rb") as f:
    ckpt = checkpoint.load(f, graphcast.CheckPoint)

  params = ckpt.params
  state = {}

  model_config = ckpt.model_config
  task_config = ckpt.task_config
  print("æ¨¡å‹æè¿°:\n", ckpt.description, "\n")
  print("æ¨¡å‹è®¸å¯ä¿¡æ¯:\n", ckpt.license, "\n")

model_config


# ## è½½å…¥ç¤ºä¾‹æ•°æ®
# 
# æœ‰å‡ ä¸ªç¤ºä¾‹æ•°æ®é›†å¯ç”¨ï¼Œåœ¨å‡ ä¸ªåæ ‡è½´ä¸Šå„ä¸ç›¸åŒï¼š
# - **æ¥æº**ï¼šfakeã€era5ã€hres
# - **åˆ†è¾¨ç‡**ï¼š0.25åº¦ã€1åº¦ã€6åº¦
# - **çº§åˆ«**ï¼š13, 37
# - **æ­¥æ•°**ï¼šåŒ…å«å¤šå°‘ä¸ªæ—¶é—´æ­¥
# 
# å¹¶éæ‰€æœ‰ç»„åˆéƒ½å¯ç”¨ã€‚
# - ç”±äºåŠ è½½å†…å­˜çš„è¦æ±‚ï¼Œè¾ƒé«˜åˆ†è¾¨ç‡åªé€‚ç”¨äºè¾ƒå°‘çš„æ­¥æ•°ã€‚
# - HRES åªæœ‰ 0.25 åº¦ï¼Œ13 ä¸ªå‹åŠ›ç­‰çº§ã€‚
# 
# æ•°æ®åˆ†è¾¨ç‡å¿…é¡»ä¸åŠ è½½çš„æ¨¡å‹ç›¸åŒ¹é…ã€‚
# 
# å¯¹åŸºç¡€æ•°æ®é›†è¿›è¡Œäº†ä¸€äº›è½¬æ¢ï¼š
# - æˆ‘ä»¬ç´¯ç§¯äº† 6 ä¸ªå°æ—¶çš„é™æ°´é‡ï¼Œè€Œä¸æ˜¯é»˜è®¤çš„ 1 ä¸ªå°æ—¶ã€‚
# - å¯¹äº HRES æ•°æ®ï¼Œæ¯ä¸ªæ—¶é—´æ­¥å¯¹åº” HRES åœ¨å‰å¯¼æ—¶é—´ 0 çš„é¢„æŠ¥ï¼Œå®é™…ä¸Šæä¾›äº† HRES çš„ "åˆå§‹åŒ–"ã€‚æœ‰å…³è¯¦ç»†æè¿°ï¼Œè¯·å‚è§ GraphCast è®ºæ–‡ä¸­çš„ HRES-fc0ã€‚è¯·æ³¨æ„ï¼ŒHRES æ— æ³•æä¾› 6 å°æ—¶çš„ç´¯ç§¯é™æ°´é‡ï¼Œå› æ­¤æˆ‘ä»¬çš„æ¨¡å‹ä»¥ HRES è¾“å…¥ä¸ä¾èµ–äºé™æ°´ã€‚ä½†ç”±äºæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥é¢„æµ‹é™æ°´ï¼Œå› æ­¤åœ¨ç¤ºä¾‹æ•°æ®ä¸­åŒ…å«äº† ERA5 é™æ°´é‡ï¼Œä»¥ä½œä¸ºåœ°é¢çœŸå®æƒ…å†µçš„ç¤ºä¾‹ã€‚
# - æˆ‘ä»¬åœ¨æ•°æ®ä¸­åŠ å…¥äº† ERA5 çš„ "toa_incident_solar_radiation"ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨ -6hã€0h å’Œ +6h è¾å°„ä½œä¸ºæ¯ 1 æ­¥é¢„æµ‹çš„å¼ºè¿«é¡¹ã€‚åœ¨è¿è¡Œä¸­ï¼Œå¦‚æœæ²¡æœ‰ç°æˆçš„ +6h è¾å°„ï¼Œå¯ä»¥ä½¿ç”¨è¯¸å¦‚ `pysolar` ç­‰è½¯ä»¶åŒ…è®¡ç®—è¾å°„ã€‚
# 

# In[13]:


# @title è·å–å’Œç­›é€‰å¯ç”¨ç¤ºä¾‹æ•°æ®çš„åˆ—è¡¨

# Rewrite by S.F. Sune, https://github.com/sfsun67.
# åœ¨"/root/data/dataset"è·¯å¾„ä¸‹æŸ¥æ‰¾ç»“æœï¼Œå¹¶åˆ—å‡º"dataset/"ä¸­æ‰€æœ‰æ–‡ä»¶çš„åç§°åˆ—è¡¨ï¼Œå»æ‰"dataset/"å‰ç¼€ã€‚

# å®šä¹‰æ•°æ®ç›®å½•ï¼Œè¯·æ›¿æ¢æˆä½ è‡ªå·±çš„ç›®å½•ã€‚
dir_path_dataset = "/root/data/dataset"

# Use glob to get all file paths in the directory
file_paths_dataset = glob.glob(os.path.join(dir_path_dataset, "*"))

# Remove the directory path and the ".../params/" prefix from each file name
dataset_file_options = [os.path.basename(path) for path in file_paths_dataset]
#print("dataset_file_options: ", dataset_file_options)

# Remove "dataset-" prefix from each file name
dataset_file_options = [name.removeprefix("dataset-") for name in dataset_file_options]


def data_valid_for_model(
    file_name: str, model_config: graphcast.ModelConfig, task_config: graphcast.TaskConfig):
  file_parts = parse_file_parts(file_name.removesuffix(".nc"))
  #print("file_parts: ", file_parts)
  return (
      model_config.resolution in (0, float(file_parts["res"])) and
      len(task_config.pressure_levels) == int(file_parts["levels"]) and
      (
          ("total_precipitation_6hr" in task_config.input_variables and
           file_parts["source"] in ("era5", "fake")) or
          ("total_precipitation_6hr" not in task_config.input_variables and
           file_parts["source"] in ("hres", "fake"))
      )
  )


dataset_file = widgets.Dropdown(
    options=[
        (", ".join([f"{k}: {v}" for k, v in parse_file_parts(option.removesuffix(".nc")).items()]), option)
        for option in dataset_file_options
        if data_valid_for_model(option, model_config, task_config)
    ],
    description="æ•°æ®æ–‡ä»¶:",
    layout={"width": "max-content"})
widgets.VBox([
    dataset_file,
    widgets.Label(value="è¿è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼ä»¥åŠ è½½æ•°æ®é›†ã€‚é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼å°†æ¸…é™¤æ‚¨çš„é€‰æ‹©ï¼Œå¹¶é‡æ–°ç­›é€‰ä¸æ‚¨çš„æ¨¡å‹åŒ¹é…çš„æ•°æ®é›†ã€‚")
])


# In[14]:


# @title åŠ è½½æ°”è±¡æ•°æ®


if not data_valid_for_model(dataset_file.value, model_config, task_config):
  raise ValueError(
      "Invalid dataset file, rerun the cell above and choose a valid dataset file.")

'''with gcs_bucket.blob(f"dataset/{dataset_file.value}").open("rb") as f:
  example_batch = xarray.load_dataset(f).compute()'''

with open(f"{dir_path_dataset}/dataset-{dataset_file.value}", "rb") as f:
  example_batch = xarray.load_dataset(f).compute()

assert example_batch.dims["time"] >= 3  # 2 for input, >=1 for targets

print(", ".join([f"{k}: {v}" for k, v in parse_file_parts(dataset_file.value.removesuffix(".nc")).items()]))

# ============================================================================
# æ‰“å°æ•°æ®é›†çš„æ—¶é—´ç‚¹ä¿¡æ¯
# ============================================================================
print("\n" + "="*80)
print("æ•°æ®é›†æ—¶é—´ç‚¹è¯¦ç»†ä¿¡æ¯")
print("="*80)

if 'time' in example_batch.coords:
    time_coord = example_batch.coords['time']
    num_times = len(time_coord)
    
    print(f"\nâ° æ—¶é—´ç‚¹æ€»æ•°: {num_times}")
    print(f"\næ—¶é—´åæ ‡ç±»å‹: {type(time_coord.values[0])}")
    print(f"æ—¶é—´åæ ‡dtype: {time_coord.values.dtype}")
    
    import pandas as pd
    
    # æ£€æŸ¥æ—¶é—´åæ ‡çš„ç±»å‹
    if np.issubdtype(time_coord.values.dtype, np.timedelta64):
        # æ—¶é—´åæ ‡æ˜¯ timedelta ç±»å‹ï¼ˆç›¸å¯¹æ—¶é—´ï¼‰
        print(f"\nâ±ï¸  æ—¶é—´åæ ‡å­˜å‚¨ä¸ºç›¸å¯¹æ—¶é—´åç§»é‡ï¼ˆtimedelta64ï¼‰")
        print(f"\næ‰€æœ‰æ—¶é—´ç‚¹ï¼ˆç›¸å¯¹åç§»ï¼‰:")
        
        # å°è¯•ä»æ•°æ®é›†å±æ€§ä¸­è·å–å‚è€ƒæ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ–‡ä»¶åä¸­çš„æ—¥æœŸ
        reference_time = None
        if hasattr(example_batch, 'attrs') and 'reference_time' in example_batch.attrs:
            reference_time = pd.Timestamp(example_batch.attrs['reference_time'])
        else:
            # ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ (dataset_file.value åº”è¯¥åŒ…å«æ—¥æœŸä¿¡æ¯)
            file_parts = parse_file_parts(dataset_file.value.removesuffix(".nc"))
            if 'date' in file_parts:
                reference_time = pd.Timestamp(file_parts['date'])
        
        for i, t in enumerate(time_coord.values):
            # è½¬æ¢ timedelta64 ä¸ºå°æ—¶æ•°
            hours_offset = t / np.timedelta64(1, 'h')
            
            if reference_time is not None:
                # å¦‚æœæœ‰å‚è€ƒæ—¶é—´ï¼Œè®¡ç®—ç»å¯¹æ—¶é—´
                abs_time = reference_time + pd.Timedelta(t)
                print(f"  [{i}] +{hours_offset:6.1f}h -> {abs_time.strftime('%Y-%m-%d %H:%M:%S')} UTC (æ˜ŸæœŸ{['ä¸€','äºŒ','ä¸‰','å››','äº”','å…­','æ—¥'][abs_time.weekday()]})")
            else:
                print(f"  [{i}] +{hours_offset:6.1f}h")
        
        # è®¡ç®—æ—¶é—´é—´éš”
        if num_times > 1:
            time_diff = time_coord.values[1] - time_coord.values[0]
            hours_diff = time_diff / np.timedelta64(1, 'h')
            print(f"\nâ±ï¸  æ—¶é—´é—´éš”: {hours_diff:.1f} å°æ—¶")
        
        if reference_time is not None:
            print(f"\nğŸ“… å‚è€ƒæ—¶é—´: {reference_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            
            # æ£€æŸ¥ç»å¯¹æ—¶é—´æ˜¯å¦åŒ…å«18:00
            print(f"\nğŸ” æ£€æŸ¥æ˜¯å¦åŒ…å«18:00çš„æ—¶é—´ç‚¹:")
            has_18_00 = False
            found_18_00_times = []
            
            for i, t in enumerate(time_coord.values):
                abs_time = reference_time + pd.Timedelta(t)
                if abs_time.hour == 18:
                    has_18_00 = True
                    found_18_00_times.append((i, abs_time))
            
            if has_18_00:
                print(f"  âœ… æ˜¯çš„ï¼Œæ•°æ®é›†åŒ…å«18:00çš„æ—¶é—´ç‚¹:")
                for idx, dt in found_18_00_times:
                    print(f"     ç´¢å¼•[{idx}]: {dt.strftime('%Y-%m-%d %H:%M:%S')} UTC")
                    if dt.date() < reference_time.date():
                        print(f"     -> è¿™æ˜¯å‰ä¸€å¤©çš„æ•°æ®")
                    elif dt.date() == reference_time.date():
                        print(f"     -> è¿™æ˜¯å½“å¤©çš„æ•°æ®")
            else:
                print(f"  âŒ å¦ï¼Œæ•°æ®é›†ä¸åŒ…å«18:00çš„æ—¶é—´ç‚¹")
    
    else:
        # æ—¶é—´åæ ‡æ˜¯ datetime64 ç±»å‹ï¼ˆç»å¯¹æ—¶é—´ï¼‰
        print(f"\nğŸ“… æ—¶é—´åæ ‡å­˜å‚¨ä¸ºç»å¯¹æ—¶é—´ï¼ˆdatetime64ï¼‰")
        print(f"\næ‰€æœ‰æ—¶é—´ç‚¹:")
        
        for i, t in enumerate(time_coord.values):
            dt = pd.Timestamp(t).to_pydatetime()
            print(f"  [{i}] {dt.strftime('%Y-%m-%d %H:%M:%S')} UTC (æ˜ŸæœŸ{['ä¸€','äºŒ','ä¸‰','å››','äº”','å…­','æ—¥'][dt.weekday()]})")
        
        # è®¡ç®—æ—¶é—´é—´éš”
        if num_times > 1:
            time_diff = time_coord.values[1] - time_coord.values[0]
            hours_diff = time_diff / np.timedelta64(1, 'h')
            print(f"\nâ±ï¸  æ—¶é—´é—´éš”: {hours_diff:.1f} å°æ—¶")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«18:00çš„æ•°æ®
        print(f"\nğŸ” æ£€æŸ¥æ˜¯å¦åŒ…å«18:00çš„æ—¶é—´ç‚¹:")
        has_18_00 = False
        found_18_00_times = []
        
        for i, t in enumerate(time_coord.values):
            dt = pd.Timestamp(t).to_pydatetime()
            if dt.hour == 18:
                has_18_00 = True
                found_18_00_times.append((i, dt))
        
        if has_18_00:
            print(f"  âœ… æ˜¯çš„ï¼Œæ•°æ®é›†åŒ…å«18:00çš„æ—¶é—´ç‚¹:")
            for idx, dt in found_18_00_times:
                print(f"     ç´¢å¼•[{idx}]: {dt.strftime('%Y-%m-%d %H:%M:%S')} UTC")
                first_dt = pd.Timestamp(time_coord.values[0]).to_pydatetime()
                if dt.date() < first_dt.date():
                    print(f"     -> è¿™æ˜¯å‰ä¸€å¤©çš„æ•°æ®")
                elif dt.date() == first_dt.date():
                    print(f"     -> è¿™æ˜¯å½“å¤©çš„æ•°æ®")
        else:
            print(f"  âŒ å¦ï¼Œæ•°æ®é›†ä¸åŒ…å«18:00çš„æ—¶é—´ç‚¹")
    
    # è¡¥å……è¯´æ˜
    print(f"\nğŸ’¡ è¯´æ˜:")
    print(f"  - GraphCast ä½¿ç”¨ç›¸é‚»ä¸¤ä¸ªæ—¶é—´ç‚¹ä½œä¸ºè¾“å…¥ (ä¾‹å¦‚: 00:00 å’Œ 06:00)")
    print(f"  - ç„¶åé¢„æµ‹æœªæ¥çš„æ—¶é—´ç‚¹ (ä¾‹å¦‚: 12:00, 18:00, ...)")
    print(f"  - ERA5 æ ‡å‡†æ•°æ®åŒ…å«: 00:00, 06:00, 12:00, 18:00 å››ä¸ªæ—¶é—´ç‚¹")

print("="*80 + "\n")

example_batch


# In[15]:


# @title é€‰æ‹©ç»˜å›¾æ•°æ®

plot_example_variable = widgets.Dropdown(
    options=example_batch.data_vars.keys(),
    value="2m_temperature",
    description="å˜é‡")
plot_example_level = widgets.Dropdown(
    options=example_batch.coords["level"].values,
    value=500,
    description="çº§åˆ«")
plot_example_robust = widgets.Checkbox(value=True, description="é²æ£’æ€§")
plot_example_max_steps = widgets.IntSlider(
    min=1, max=example_batch.dims["time"], value=example_batch.dims["time"],
    description="æœ€å¤§æ­¥")

widgets.VBox([
    plot_example_variable,
    plot_example_level,
    plot_example_robust,
    plot_example_max_steps,
    widgets.Label(value="è¿è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼ä»¥ç»˜åˆ¶æ•°æ®ã€‚é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼å°†æ¸…é™¤æ‚¨çš„é€‰æ‹©ã€‚")
])


# In[16]:


# @title ç»˜åˆ¶ç¤ºä¾‹æ•°æ®


plot_size = 7

data = {
    " ": scale(select(example_batch, plot_example_variable.value, plot_example_level.value, plot_example_max_steps.value),
              robust=plot_example_robust.value),
}
fig_title = plot_example_variable.value
if "ç­‰çº§" in example_batch[plot_example_variable.value].coords:
  fig_title += f" at {plot_example_level.value} hPa"

plot_data(data, fig_title, plot_size, plot_example_robust.value)


# In[17]:


# @title é€‰æ‹©è¦æå–çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®

train_steps = widgets.IntSlider(
    value=1, min=1, max=example_batch.sizes["time"]-2, description="è®­ç»ƒæ­¥æ•°")
eval_steps = widgets.IntSlider(
    value=example_batch.sizes["time"]-2, min=1, max=example_batch.sizes["time"]-2, description="è¯„ä¼°æ­¥æ•°")

widgets.VBox([
    train_steps,
    eval_steps,
    widgets.Label(value="è¿è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼ä»¥æå–æ•°æ®ã€‚é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼å°†æ¸…é™¤æ‚¨çš„é€‰æ‹©ã€‚")
])


# In[18]:


# @title æå–è®­ç»ƒå’Œè¯„ä¼°æ•°æ®

train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(
    example_batch, target_lead_times=slice("6h", f"{train_steps.value*6}h"),
    **dataclasses.asdict(task_config))

eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(
    example_batch, target_lead_times=slice("6h", f"{eval_steps.value*6}h"),
    **dataclasses.asdict(task_config))

print("æ‰€æœ‰ç¤ºä¾‹ï¼š  ", example_batch.dims.mapping)
print("è®­ç»ƒè¾“å…¥ï¼š  ", train_inputs.dims.mapping)
print("è®­ç»ƒç›®æ ‡ï¼š ", train_targets.dims.mapping)
print("è®­ç»ƒå¼ºè¿«ï¼š", train_forcings.dims.mapping)
print("è¯„ä¼°è¾“å…¥ï¼š   ", eval_inputs.dims.mapping)
print("è¯„ä¼°ç›®æ ‡ï¼š  ", eval_targets.dims.mapping)
print("è¯„ä¼°å¼ºè¿«é¡¹: ", eval_forcings.dims.mapping)

# ============================================================================
# æ‰“å° train_inputs å’Œ train_targets çš„æ—¶é—´ä¿¡æ¯
# ============================================================================
print("\n" + "="*80)
print("è®­ç»ƒæ•°æ®æ—¶é—´ä¿¡æ¯")
print("="*80)

import pandas as pd

# ä»æ–‡ä»¶åä¸­æå–å‚è€ƒæ—¥æœŸ
file_parts = parse_file_parts(dataset_file.value.removesuffix(".nc"))
reference_time = None
if 'date' in file_parts:
    reference_time = pd.Timestamp(file_parts['date'])
    print(f"\nğŸ“… æ•°æ®é›†å‚è€ƒæ—¥æœŸ: {reference_time.strftime('%Y-%m-%d')}")

# æ‰“å° train_inputs çš„æ—¶é—´
print("\nğŸ”¹ train_inputs åŒ…å«çš„æ—¶é—´ç‚¹:")
if 'time' in train_inputs.coords:
    train_input_times = train_inputs.coords['time'].values
    print(f"   æ—¶é—´ç‚¹æ•°é‡: {len(train_input_times)}")
    
    for i, t in enumerate(train_input_times):
        if np.issubdtype(train_input_times.dtype, np.timedelta64):
            hours_offset = t / np.timedelta64(1, 'h')
            if reference_time is not None:
                abs_time = reference_time + pd.Timedelta(t)
                print(f"   [{i}] ç›¸å¯¹æ—¶é—´: +{hours_offset:6.1f}h -> ç»å¯¹æ—¶é—´: {abs_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            else:
                print(f"   [{i}] ç›¸å¯¹æ—¶é—´: +{hours_offset:6.1f}h")
        else:
            dt = pd.Timestamp(t).to_pydatetime()
            print(f"   [{i}] {dt.strftime('%Y-%m-%d %H:%M:%S')} UTC")
else:
    print("   âš ï¸ train_inputs æ²¡æœ‰ time åæ ‡")

# æ‰“å° train_targets çš„æ—¶é—´
print("\nğŸ”¹ train_targets åŒ…å«çš„æ—¶é—´ç‚¹:")
if 'time' in train_targets.coords:
    train_target_times = train_targets.coords['time'].values
    print(f"   æ—¶é—´ç‚¹æ•°é‡: {len(train_target_times)}")
    
    for i, t in enumerate(train_target_times):
        if np.issubdtype(train_target_times.dtype, np.timedelta64):
            hours_offset = t / np.timedelta64(1, 'h')
            if reference_time is not None:
                abs_time = reference_time + pd.Timedelta(t)
                print(f"   [{i}] ç›¸å¯¹æ—¶é—´: +{hours_offset:6.1f}h -> ç»å¯¹æ—¶é—´: {abs_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            else:
                print(f"   [{i}] ç›¸å¯¹æ—¶é—´: +{hours_offset:6.1f}h")
        else:
            dt = pd.Timestamp(t).to_pydatetime()
            print(f"   [{i}] {dt.strftime('%Y-%m-%d %H:%M:%S')} UTC")
else:
    print("   âš ï¸ train_targets æ²¡æœ‰ time åæ ‡")

# ============================================================================
# æ‰“å° eval_inputs å’Œ eval_targets çš„æ—¶é—´ä¿¡æ¯
# ============================================================================
print("\n" + "="*80)
print("è¯„ä¼°æ•°æ®æ—¶é—´ä¿¡æ¯")
print("="*80)

# æ‰“å° eval_inputs çš„æ—¶é—´
print("\nğŸ”¹ eval_inputs åŒ…å«çš„æ—¶é—´ç‚¹:")
if 'time' in eval_inputs.coords:
    eval_input_times = eval_inputs.coords['time'].values
    print(f"   æ—¶é—´ç‚¹æ•°é‡: {len(eval_input_times)}")
    
    for i, t in enumerate(eval_input_times):
        if np.issubdtype(eval_input_times.dtype, np.timedelta64):
            hours_offset = t / np.timedelta64(1, 'h')
            if reference_time is not None:
                abs_time = reference_time + pd.Timedelta(t)
                print(f"   [{i}] ç›¸å¯¹æ—¶é—´: +{hours_offset:6.1f}h -> ç»å¯¹æ—¶é—´: {abs_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            else:
                print(f"   [{i}] ç›¸å¯¹æ—¶é—´: +{hours_offset:6.1f}h")
        else:
            dt = pd.Timestamp(t).to_pydatetime()
            print(f"   [{i}] {dt.strftime('%Y-%m-%d %H:%M:%S')} UTC")
else:
    print("   âš ï¸ eval_inputs æ²¡æœ‰ time åæ ‡")

# æ‰“å° eval_targets çš„æ—¶é—´
print("\nğŸ”¹ eval_targets åŒ…å«çš„æ—¶é—´ç‚¹:")
if 'time' in eval_targets.coords:
    eval_target_times = eval_targets.coords['time'].values
    print(f"   æ—¶é—´ç‚¹æ•°é‡: {len(eval_target_times)}")
    
    for i, t in enumerate(eval_target_times):
        if np.issubdtype(eval_target_times.dtype, np.timedelta64):
            hours_offset = t / np.timedelta64(1, 'h')
            if reference_time is not None:
                abs_time = reference_time + pd.Timedelta(t)
                print(f"   [{i}] ç›¸å¯¹æ—¶é—´: +{hours_offset:6.1f}h -> ç»å¯¹æ—¶é—´: {abs_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            else:
                print(f"   [{i}] ç›¸å¯¹æ—¶é—´: +{hours_offset:6.1f}h")
        else:
            dt = pd.Timestamp(t).to_pydatetime()
            print(f"   [{i}] {dt.strftime('%Y-%m-%d %H:%M:%S')} UTC")
else:
    print("   âš ï¸ eval_targets æ²¡æœ‰ time åæ ‡")

# è¡¥å……è¯´æ˜
print("\nğŸ’¡ è¯´æ˜:")
print("   - eval_inputs å›ºå®šåŒ…å«2ä¸ªè¿ç»­æ—¶é—´ç‚¹ (ç”¨äºé¢„æµ‹çš„è¾“å…¥)")
print("   - eval_targets åŒ…å«å¤šä¸ªé¢„æµ‹ç›®æ ‡æ—¶é—´ç‚¹ (ç”± eval_steps å‚æ•°å†³å®š)")
print("   - æ—¶é—´é—´éš”ä¸º6å°æ—¶")
print("="*80 + "\n")


# In[19]:


# @title åŠ è½½è§„èŒƒåŒ–æ•°æ®
# Rewrite by S.F. Sune, https://github.com/sfsun67.
dir_path_stats = "/root/data/stats"

with open(f"{dir_path_stats}/stats-diffs_stddev_by_level.nc", "rb") as f:
  diffs_stddev_by_level = xarray.load_dataset(f).compute()
with open(f"{dir_path_stats}/stats-mean_by_level.nc", "rb") as f:
  mean_by_level = xarray.load_dataset(f).compute()
with open(f"{dir_path_stats}/stats-stddev_by_level.nc", "rb") as f:
  stddev_by_level = xarray.load_dataset(f).compute()


# In[20]:


# @title æ„å»º jitted å‡½æ•°ï¼Œå¹¶å¯èƒ½åˆå§‹åŒ–éšæœºæƒé‡
# æ„å»ºæ¨¡å‹å¹¶åˆå§‹åŒ–æƒé‡

# æ¨¡å‹ç»„ç½‘
def construct_wrapped_graphcast(
    model_config: graphcast.ModelConfig,
    task_config: graphcast.TaskConfig):
  """Constructs and wraps the GraphCast Predictor."""
  # Deeper one-step predictor.
  predictor = graphcast.GraphCast(model_config, task_config)

  # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to
  # from/to float32 to/from BFloat16.
  predictor = casting.Bfloat16Cast(predictor)

  # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from
  # BFloat16 happens after applying normalization to the inputs/targets.
  predictor = normalization.InputsAndResiduals(
      predictor,
      diffs_stddev_by_level=diffs_stddev_by_level,
      mean_by_level=mean_by_level,
      stddev_by_level=stddev_by_level)

  # Wraps everything so the one-step model can produce trajectories.
  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)
  return predictor

# å‰å‘è¿ç®—
@hk.transform_with_state
def run_forward(model_config, task_config, inputs, targets_template, forcings):
  predictor = construct_wrapped_graphcast(model_config, task_config)
  return predictor(inputs, targets_template=targets_template, forcings=forcings)

# è®¡ç®—æŸå¤±å‡½æ•°
@hk.transform_with_state    # used to convert a pure function into a stateful function
def loss_fn(model_config, task_config, inputs, targets, forcings):
  predictor = construct_wrapped_graphcast(model_config, task_config)    # constructs and wraps a GraphCast Predictor, which is a model used for making predictions in a graph-based machine learning task.
  loss, diagnostics = predictor.loss(inputs, targets, forcings)
  return xarray_tree.map_structure(
      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),
      (loss, diagnostics))

# è®¡ç®—æ¢¯åº¦
def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):
  def _aux(params, state, i, t, f):
    (loss, diagnostics), next_state = loss_fn.apply(
        params, state, jax.random.PRNGKey(0), model_config, task_config,
        i, t, f)
    return loss, (diagnostics, next_state)
  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(
      _aux, has_aux=True)(params, state, inputs, targets, forcings)
  return loss, diagnostics, next_state, grads

# Jax doesn't seem to like passing configs as args through the jit. Passing it
# in via partial (instead of capture by closure) forces jax to invalidate the
# jit cache if you change configs.
def with_configs(fn):
  return functools.partial(
      fn, model_config=model_config, task_config=task_config)

# Always pass params and state, so the usage below are simpler
def with_params(fn):
  return functools.partial(fn, params=params, state=state)

# Our models aren't stateful, so the state is always empty, so just return the
# predictions. This is requiredy by our rollout code, and generally simpler.
def drop_state(fn):
  return lambda **kw: fn(**kw)[0]

init_jitted = jax.jit(with_configs(run_forward.init))

if params is None:
  params, state = init_jitted(
      rng=jax.random.PRNGKey(0),
      inputs=train_inputs,
      targets_template=train_targets,
      forcings=train_forcings)

loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))
grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))
run_forward_jitted = drop_state(with_params(jax.jit(with_configs(
    run_forward.apply))))


# # è¿è¡Œæ¨¡å‹
# 
# è¯·æ³¨æ„ï¼Œç¬¬ä¸€æ¬¡è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ï¼ˆå¯èƒ½å‡ åˆ†é’Ÿï¼‰ï¼Œå› ä¸ºè¿™åŒ…æ‹¬ä»£ç ç¼–è¯‘çš„æ—¶é—´ã€‚ç¬¬äºŒæ¬¡è¿è¡Œæ—¶é€Ÿåº¦ä¼šæ˜æ˜¾åŠ å¿«ã€‚
# 
# è¿™å°†ä½¿ç”¨ python å¾ªç¯è¿­ä»£é¢„æµ‹æ­¥éª¤ï¼Œå…¶ä¸­ 1 æ­¥çš„é¢„æµ‹æ˜¯å›ºå®šçš„ã€‚è¿™æ¯”ä¸‹é¢çš„è®­ç»ƒæ­¥éª¤å¯¹å†…å­˜çš„è¦æ±‚è¦ä½ï¼Œåº”è¯¥å¯ä»¥ä½¿ç”¨å°å‹ GraphCast æ¨¡å‹å¯¹ 1 åº¦åˆ†è¾¨ç‡æ•°æ®è¿›è¡Œ 4 æ­¥é¢„æµ‹ã€‚

# In[21]:


# @æ ‡é¢˜ é€’å½’è®¡ç®—ï¼ˆåœ¨ python ä¸­çš„å¾ªç¯ï¼‰
import pickle
assert model_config.resolution in (0, 360. / eval_inputs.sizes["lon"]), (
  "Model resolution doesn't match the data resolution. You likely want to "
  "re-filter the dataset list, and download the correct data.")

print("Inputs:  ", eval_inputs.dims.mapping)
print("Targets: ", eval_targets.dims.mapping)
print("Forcings:", eval_forcings.dims.mapping)

# with open('eval_inputs.pkl', 'wb') as file:
#     pickle.dump(eval_inputs, file)
# with open('eval_targets.pkl', 'wb') as file:
#     pickle.dump(eval_targets, file)
# with open('eval_forcings.pkl', 'wb') as file:
#     pickle.dump(eval_forcings, file)


# with jax.disable_jit():
predictions = rollout.chunked_prediction(
    run_forward_jitted,
    rng=jax.random.PRNGKey(0),
    inputs=eval_inputs,
    targets_template=eval_targets * np.nan,
    forcings=eval_forcings)
predictions


with open('predictions.pkl', 'wb') as file:
    pickle.dump(predictions, file)


# In[22]:


# @title é€‰æ‹©è¦ç»˜åˆ¶çš„é¢„æµ‹ç»“æœ

plot_pred_variable = widgets.Dropdown(
    options=predictions.data_vars.keys(),
    value="2m_temperature",
    description="å˜é‡")
plot_pred_level = widgets.Dropdown(
    options=predictions.coords["level"].values,
    value=500,
    description="çº§åˆ«")
plot_pred_robust = widgets.Checkbox(value=True, description="é²æ£’æ€§")
plot_pred_max_steps = widgets.IntSlider(
    min=1,
    max=predictions.dims["time"],
    value=predictions.dims["time"],
    description="æœ€å¤§æ­¥")

widgets.VBox([
    plot_pred_variable,
    plot_pred_level,
    plot_pred_robust,
    plot_pred_max_steps,
    widgets.Label(value="è¿è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼ï¼Œç»˜åˆ¶é¢„æµ‹ç»“æœã€‚é‡æ–°è¿è¡Œè¯¥å•å…ƒæ ¼å°†æ¸…é™¤æ‚¨çš„é€‰æ‹©ã€‚")
])


# In[23]:
# @title æ‰“å°è¾“å…¥æ•°æ®é›†çš„æ ¼å¼

print("=== è¾“å…¥æ•°æ®é›†æ ¼å¼ä¿¡æ¯ ===")
print("\n1. eval_inputs æ ¼å¼:")
print(f"   ç±»å‹: {type(eval_inputs)}")
print(f"   ç»´åº¦: {eval_inputs.dims}")
print(f"   åæ ‡: {list(eval_inputs.coords.keys())}")
print(f"   æ•°æ®å˜é‡: {list(eval_inputs.data_vars.keys())}")
print(f"   å½¢çŠ¶: {eval_inputs.sizes}")

print("\n2. eval_targets æ ¼å¼:")
print(f"   ç±»å‹: {type(eval_targets)}")
print(f"   ç»´åº¦: {eval_targets.dims}")
print(f"   åæ ‡: {list(eval_targets.coords.keys())}")
print(f"   æ•°æ®å˜é‡: {list(eval_targets.data_vars.keys())}")
print(f"   å½¢çŠ¶: {eval_targets.sizes}")

print("\n3. eval_forcings æ ¼å¼:")
print(f"   ç±»å‹: {type(eval_forcings)}")
print(f"   ç»´åº¦: {eval_forcings.dims}")
print(f"   åæ ‡: {list(eval_forcings.coords.keys())}")
print(f"   æ•°æ®å˜é‡: {list(eval_forcings.data_vars.keys())}")
print(f"   å½¢çŠ¶: {eval_forcings.sizes}")


# === è¾“å…¥æ•°æ®é›†æ ¼å¼ä¿¡æ¯ ===

# 1. eval_inputs æ ¼å¼:
#    ç±»å‹: <class 'xarray.core.dataset.Dataset'>
#    ç»´åº¦: Frozen({'batch': 1, 'time': 2, 'lat': 721, 'lon': 1440, 'level': 37})
#    åæ ‡: ['lon', 'lat', 'level', 'time']
#    æ•°æ®å˜é‡: ['2m_temperature', 'mean_sea_level_pressure', '10m_v_component_of_wind', '10m_u_component_of_wind', 'total_precipitation_6hr', 'temperature', 'geopotential', 'u_component_of_wind', 'v_component_of_wind', 'vertical_velocity', 'specific_humidity', 'toa_incident_solar_radiation', 'year_progress_sin', 'year_progress_cos', 'day_progress_sin', 'day_progress_cos', 'geopotential_at_surface', 'land_sea_mask']
#    å½¢çŠ¶: Frozen({'batch': 1, 'time': 2, 'lat': 721, 'lon': 1440, 'level': 37})

# 2. eval_targets æ ¼å¼:
#    ç±»å‹: <class 'xarray.core.dataset.Dataset'>
#    ç»´åº¦: Frozen({'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440, 'level': 37})
#    åæ ‡: ['lon', 'lat', 'level', 'time']
#    æ•°æ®å˜é‡: ['2m_temperature', 'mean_sea_level_pressure', '10m_v_component_of_wind', '10m_u_component_of_wind', 'total_precipitation_6hr', 'temperature', 'geopotential', 'u_component_of_wind', 'v_component_of_wind', 'vertical_velocity', 'specific_humidity']
#    å½¢çŠ¶: Frozen({'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440, 'level': 37})

# 3. eval_forcings æ ¼å¼:
#    ç±»å‹: <class 'xarray.core.dataset.Dataset'>
#    ç»´åº¦: Frozen({'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440})
#    åæ ‡: ['lon', 'lat', 'time']
#    æ•°æ®å˜é‡: ['toa_incident_solar_radiation', 'year_progress_sin', 'year_progress_cos', 'day_progress_sin', 'day_progress_cos']
#    å½¢çŠ¶: Frozen({'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440})








# # è®¡ç®—æƒé‡

# In[ ]:


print(train_inputs.dims)  # æŸ¥çœ‹ train_inputs çš„ç»´åº¦
print(train_forcings.dims)  # æŸ¥çœ‹ train_forcings çš„ç»´åº¦


# In[ ]:


# import xarray as xr
# import jax
# import numpy as np

# # å‡½æ•°ï¼šé€é€šé“ç½®é›¶å¹¶è®¡ç®—æ‰€æœ‰é¢„æµ‹ç»“æœçš„å˜åŒ–
# def zero_out_channel(inputs, forcings, channel_name, variable='inputs'):
#     """å°†æŒ‡å®šé€šé“çš„è¾“å…¥å˜é‡æˆ–å¼ºè¿«é¡¹ç½®é›¶ï¼Œå¹¶è®¡ç®—æ‰€æœ‰é¢„æµ‹ç»“æœçš„å˜åŒ–"""
#     if variable == 'inputs':
#         inputs_copy = inputs.copy()  # å¤åˆ¶inputs
#         inputs_copy[channel_name].data = np.zeros_like(inputs_copy[channel_name].data)  # å°†æŒ‡å®šé€šé“çš„è¾“å…¥ç½®ä¸ºé›¶
#         forcings_copy = forcings  # forcingsä¸å˜
#     elif variable == 'forcings':
#         forcings_copy = forcings.copy()  # å¤åˆ¶forcings
#         forcings_copy[channel_name].data = np.zeros_like(forcings_copy[channel_name].data)  # å°†æŒ‡å®šé€šé“çš„å¼ºè¿«é¡¹ç½®ä¸ºé›¶
#         inputs_copy = inputs  # inputsä¸å˜

#     # è®¡ç®—æ–°çš„æ‰€æœ‰é¢„æµ‹ç»“æœ
#     predictions = rollout.chunked_prediction(
#         run_forward_jitted,
#         rng=jax.random.PRNGKey(0),
#         inputs=inputs_copy,
#         targets_template=train_targets * np.nan,
#         forcings=forcings_copy
#     )

#     return predictions

# # è®¡ç®—åŸå§‹é¢„æµ‹ç»“æœï¼ˆé®æŒ¡å‰ï¼‰
# def get_original_predictions(inputs, forcings):
#     """è®¡ç®—ä¸åšä»»ä½•é®æŒ¡çš„åŸå§‹é¢„æµ‹ç»“æœ"""
#     return rollout.chunked_prediction(
#         run_forward_jitted,
#         rng=jax.random.PRNGKey(0),
#         inputs=inputs,
#         targets_template=train_targets * np.nan,
#         forcings=forcings
#     )

# # è¯„ä¼°æ¯ä¸ªå˜é‡å¯¹æ‰€æœ‰é¢„æµ‹ç»“æœçš„å½±å“
# def evaluate_variable_importance(train_inputs, train_forcings, variable_type='inputs'):
#     channel_importance = {}  # ç”¨æ¥å­˜å‚¨æ¯ä¸ªå˜é‡å¯¹æ‰€æœ‰é¢„æµ‹çš„å½±å“å€¼

#     # è·å–åŸå§‹é¢„æµ‹ç»“æœ
#     original_predictions = get_original_predictions(train_inputs, train_forcings)

#     # è®¡ç®—æ¯ä¸ªå˜é‡çš„å½±å“
#     if variable_type == 'inputs':
#         channels = list(train_inputs.data_vars)  # è·å–inputsä¸­çš„æ‰€æœ‰å˜é‡å
#     elif variable_type == 'forcings':
#         channels = list(train_forcings.data_vars)  # è·å–forcingsä¸­çš„æ‰€æœ‰å˜é‡å

#     # å¯¹æ¯ä¸ªé€šé“è¿›è¡Œç½®é›¶æ“ä½œï¼Œå¹¶è®¡ç®—é¢„æµ‹ç»“æœå˜åŒ–
#     for channel_name in channels:
#         # é€é€šé“ç½®é›¶å¹¶è®¡ç®—æ‰€æœ‰é¢„æµ‹ç»“æœ
#         predictions_with_zero = zero_out_channel(train_inputs, train_forcings, channel_name, variable=variable_type)

#         # è®¡ç®—é¢„æµ‹ç»“æœçš„å˜åŒ–ï¼ˆå¯¹äºæ‰€æœ‰é¢„æµ‹å˜é‡ï¼‰
#         prediction_diff = {}
#         for var_name in original_predictions:
#             # è®¡ç®—è¯¥é¢„æµ‹å˜é‡çš„å˜åŒ–
#             prediction_diff[var_name] = np.abs(original_predictions[var_name] - predictions_with_zero[var_name])

#         # è®¡ç®—æ‰€æœ‰è¾“å‡ºå˜é‡çš„å˜åŒ–å¹³å‡å€¼ï¼Œä½œä¸ºè¯¥é€šé“çš„é‡è¦æ€§åº¦é‡
#         total_diff = np.mean([np.mean(diff) for diff in prediction_diff.values()])  # å¯¹æ‰€æœ‰å˜é‡çš„å·®å¼‚å–å¹³å‡
#         channel_importance[channel_name] = total_diff  # å­˜å‚¨æ¯ä¸ªé€šé“çš„æ€»å½±å“å€¼

#     return channel_importance

# # è¯„ä¼°ä¸åŒå¤§æ°”å˜é‡çš„å½±å“
# inputs_importance = evaluate_variable_importance(train_inputs, train_forcings, variable_type='inputs')
# forcings_importance = evaluate_variable_importance(train_inputs, train_forcings, variable_type='forcings')

# è¾“å‡ºæ¯ä¸ªå˜é‡å¯¹æ‰€æœ‰é¢„æµ‹ç»“æœçš„å½±å“
# print(inputs_importance)
# print(forcings_importance)
# inputs_importance
# forcings_importance


# 2m_temperature
# mean_sea_level_pressure
# 10m_v_component_of_wind
# 10m_u_component_of_wind
# total_precipitation_6hr
# temperature
# geopotential
# u_component_of_wind
# v_component_of_wind
# vertical_velocity
# specific_humidity
# toa_incident_solar_radiation
# year_progress_sin
# year_progress_cos
# day_progress_sin
# day_progress_cos
# geopotential_at_surface
# land_sea_mask


# # è®­ç»ƒæ¨¡å‹
# 
# ä»¥ä¸‹æ“ä½œéœ€è¦å¤§é‡å†…å­˜ï¼Œè€Œä¸”æ ¹æ®æ‰€ä½¿ç”¨çš„åŠ é€Ÿå™¨ï¼Œåªèƒ½åœ¨ä½åˆ†è¾¨ç‡æ•°æ®ä¸Šæ‹Ÿåˆå¾ˆå°çš„ "éšæœº "æ¨¡å‹ã€‚å®ƒä½¿ç”¨ä¸Šé¢é€‰æ‹©çš„è®­ç»ƒæ­¥æ•°ã€‚
# 
# ç¬¬ä¸€æ¬¡æ‰§è¡Œå•å…ƒéœ€è¦æ›´å¤šæ—¶é—´ï¼Œå› ä¸ºå…¶ä¸­åŒ…æ‹¬å‡½æ•°çš„ jit æ—¶é—´ã€‚

# In[ ]:


# # @title æŸå¤±è®¡ç®—ï¼ˆå¤šæ­¥éª¤é€’å½’ï¼ˆè‡ªå›å½’ï¼‰æŸå¤±ï¼‰
# loss, diagnostics = loss_fn_jitted(
#     rng=jax.random.PRNGKey(0),
#     inputs=train_inputs,
#     targets=train_targets,
#     forcings=train_forcings)

# print("Loss:", float(loss))


# In[ ]:


# # @title æ¢¯åº¦è®¡ç®—ï¼ˆé€šè¿‡æ—¶é—´è¿›è¡Œåæ¨ï¼‰
# loss, diagnostics, next_state, grads = grads_fn_jitted(
#     inputs=train_inputs,
#     targets=train_targets,
#     forcings=train_forcings)
# mean_grad = np.mean(jax.tree_util.tree_flatten(jax.tree_util.tree_map(lambda x: np.abs(x).mean(), grads))[0])
# print(f"Loss: {loss:.4f}, Mean |grad|: {mean_grad:.6f}")


# In[ ]:


# # @title é€’å½’ï¼ˆè‡ªå›å½’ï¼‰æ¨å‡ºï¼ˆåœ¨ JAX ä¸­ä¿æŒå¾ªç¯ï¼‰
# print("Inputs:  ", train_inputs.dims.mapping)
# print("Targets: ", train_targets.dims.mapping)
# print("Forcings:", train_forcings.dims.mapping)

# predictions = run_forward_jitted(
#     rng=jax.random.PRNGKey(0),
#     inputs=train_inputs,
#     targets_template=train_targets * np.nan,
#     forcings=train_forcings)
# predictions



# In[ ]:

# # ä½¿ç”¨å¹³æµæ–¹ç¨‹è®¡ç®—6å°æ—¶é¢„æŠ¥

# @title å¹³æµæ–¹ç¨‹å®ç°å’Œæµ‹è¯•

# å¯¼å…¥å¹³æµè®¡ç®—æ¨¡å—
import sys
import os
import importlib

# ç¡®ä¿å½“å‰ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# æ¸…é™¤å¯èƒ½çš„æ¨¡å—ç¼“å­˜
if 'advection_calculation' in sys.modules:
    del sys.modules['advection_calculation']

try:
    # å¯¼å…¥æ¨¡å—
    import advection_calculation
    
    # å¯¼å…¥æ‰€éœ€å‡½æ•°
    from advection_calculation import (
        calculate_advection_forecast, 
        calculate_enhanced_advection_forecast,
        calculate_correlation as advection_calculate_correlation, 
        print_correlation_results as advection_print_correlation_results
    )
    
    # å¯¼å…¥åœ°è¡¨èƒ½é‡å¹³è¡¡æ¨¡å—
    import surface_energy_balance
    from surface_energy_balance import (
        calculate_surface_energy_balance_forecast,
        calculate_correlation as seb_calculate_correlation,
        print_correlation_results as seb_print_correlation_results
    )
    
    print("å¹³æµæ–¹ç¨‹æ¨¡å—å¯¼å…¥æˆåŠŸï¼")
    print("åœ°è¡¨èƒ½é‡å¹³è¡¡æ¨¡å—å¯¼å…¥æˆåŠŸï¼")
    
    # æ¼”ç¤ºå…¬å¼è¯´æ˜
    print("\n" + "="*80)
    print("å¯ç”¨çš„ç‰©ç†æ¨¡å‹å…¬å¼è¯´æ˜")
    print("="*80)
    
    print("\n1. å¹³æµæ–¹ç¨‹ (Advection Equation):")
    print("ä½¿ç”¨çš„å…¬å¼: T(t+Î”t)(x,y,z) = T(t)(x,y,z) - Î”t[uâˆ‚T/âˆ‚x + vâˆ‚T/âˆ‚y + wâˆ‚T/âˆ‚z]")
    print("å…¶ä¸­:")
    print("  T(t+Î”t) - 6å°æ—¶åçš„æ¸©åº¦åœº")
    print("  T(t)    - å½“å‰æ—¶åˆ»çš„æ¸©åº¦åœº") 
    print("  Î”t      - æ—¶é—´æ­¥é•¿ (6å°æ—¶ = 21600ç§’)")
    print("  u       - xæ–¹å‘(ç»åº¦)é£é€Ÿåˆ†é‡ (u_component_of_wind)")
    print("  v       - yæ–¹å‘(çº¬åº¦)é£é€Ÿåˆ†é‡ (v_component_of_wind)")
    print("  w       - zæ–¹å‘(å‚ç›´)é£é€Ÿåˆ†é‡ (vertical_velocity)")
    print("  âˆ‚T/âˆ‚x   - æ¸©åº¦åœ¨ç»åº¦æ–¹å‘çš„æ¢¯åº¦")
    print("  âˆ‚T/âˆ‚y   - æ¸©åº¦åœ¨çº¬åº¦æ–¹å‘çš„æ¢¯åº¦")
    print("  âˆ‚T/âˆ‚z   - æ¸©åº¦åœ¨å‚ç›´æ–¹å‘çš„æ¢¯åº¦")
    
    print("\n2. åœ°è¡¨èƒ½é‡å¹³è¡¡ (Surface Energy Balance):")
    print("ä½¿ç”¨çš„å…¬å¼: Î”T = (R_n - H - LE - G) Â· dt / (Ï Â· c_p Â· z_heat)")
    print("å…¶ä¸­:")
    print("  R_n     - å‡€è¾å°„ (W mâ»Â²)")
    print("  H       - æ„Ÿçƒ­é€šé‡ (W mâ»Â²)")
    print("  LE      - æ½œçƒ­é€šé‡ (W mâ»Â²)")
    print("  G       - åœ°çƒ­é€šé‡ (W mâ»Â²)")
    print("  Ï       - ç©ºæ°”å¯†åº¦ (kg mâ»Â³)")
    print("  c_p     - å®šå‹æ¯”çƒ­ (J kgâ»Â¹ Kâ»Â¹)")
    print("  z_heat  - æœ‰æ•ˆçƒ­å®¹é‡æ·±åº¦ (m)")
    print("  dt      - æ—¶é—´æ­¥é•¿ (6å°æ—¶ = 21600ç§’)")
    
except Exception as e:
    print(f"å¯¼å…¥å¤±è´¥: {e}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"Pythonè·¯å¾„: {sys.path[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ªè·¯å¾„
    print(f"advection_calculation.py æ˜¯å¦å­˜åœ¨: {os.path.exists('advection_calculation.py')}")
    
    # å°è¯•åˆ—å‡ºå½“å‰ç›®å½•çš„æ–‡ä»¶
    try:
        files = [f for f in os.listdir('.') if f.endswith('.py')]
        print(f"å½“å‰ç›®å½•çš„Pythonæ–‡ä»¶: {files}")
    except:
        pass


# In[ ]:


# @title ä½¿ç”¨ç‰©ç†å…¬å¼è®¡ç®—6å°æ—¶é¢„æŠ¥å¹¶æ¯”è¾ƒç›¸å…³æ€§

try:
    # é€‰æ‹©ä½¿ç”¨çš„æ–¹æ³• - ç›®å‰ä½¿ç”¨åœ°è¡¨èƒ½é‡å¹³è¡¡æ–¹æ³•
    print("="*80)
    print("ä½¿ç”¨åœ°è¡¨èƒ½é‡å¹³è¡¡è®¡ç®—6å°æ—¶é¢„æŠ¥")
    print("="*80)
    
    # ä½¿ç”¨åœ°è¡¨èƒ½é‡å¹³è¡¡æ–¹æ³•
    seb_results = calculate_surface_energy_balance_forecast(eval_inputs)
    
    print(f"åœ°è¡¨èƒ½é‡å¹³è¡¡è®¡ç®—å®Œæˆï¼Œå¾—åˆ° {len(seb_results)} ä¸ªé¢„æŠ¥å˜é‡:")
    for var_name, data in seb_results.items():
        print(f"  {var_name}: {data.shape}")
    
    # è®¡ç®—ä¸GraphCastç»“æœçš„ç›¸å…³æ€§
    print("\n" + "="*80)
    print("è®¡ç®—ä¸GraphCasté¢„æµ‹ç»“æœçš„ç›¸å…³æ€§")
    print("="*80)
    
    correlations = seb_calculate_correlation(seb_results, predictions)
    seb_print_correlation_results(correlations)
    
    # å°†ç»“æœä¿å­˜åˆ°å˜é‡ä¸­ä¾›åç»­ç»˜å›¾ä½¿ç”¨
    physics_results = seb_results
    physics_method_name = "Surface Energy Balance"
    
    # =========== å¹³æµæ–¹ç¨‹æ–¹æ³•ï¼ˆå·²æ³¨é‡Šï¼‰ ===========
    # # ä½¿ç”¨å¹³æµæ–¹ç¨‹è®¡ç®—6å°æ—¶é¢„æŠ¥
    # print("="*80)
    # print("ä½¿ç”¨å¹³æµæ–¹ç¨‹è®¡ç®—6å°æ—¶é¢„æŠ¥")
    # print("="*80)
    # 
    # advection_results = calculate_enhanced_advection_forecast(eval_inputs)
    # 
    # print(f"å¹³æµæ–¹ç¨‹è®¡ç®—å®Œæˆï¼Œå¾—åˆ° {len(advection_results)} ä¸ªé¢„æŠ¥å˜é‡:")
    # for var_name, data in advection_results.items():
    #     print(f"  {var_name}: {data.shape}")
    # 
    # # è®¡ç®—ä¸GraphCastç»“æœçš„ç›¸å…³æ€§
    # print("\n" + "="*80)
    # print("è®¡ç®—ä¸GraphCasté¢„æµ‹ç»“æœçš„ç›¸å…³æ€§")
    # print("="*80)
    # 
    # correlations = advection_calculate_correlation(advection_results, predictions)
    # advection_print_correlation_results(correlations)
    # 
    # # å°†ç»“æœä¿å­˜åˆ°å˜é‡ä¸­ä¾›åç»­ç»˜å›¾ä½¿ç”¨
    # physics_results = advection_results
    # physics_method_name = "Advection"
    
except Exception as e:
    print(f"è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()


# In[ ]:
# @title ç»˜åˆ¶è¾“å…¥ã€é¢„æµ‹ã€ç‰©ç†å…¬å¼è®¡ç®—ç»“æœåŠå·®å¼‚å¯¹æ¯”å›¾

try:
    # é€‰æ‹©è¦å¯è§†åŒ–çš„å˜é‡
    variables_to_plot = ['2m_temperature', 'temperature']
    
    for var_name in variables_to_plot:
        if var_name in physics_results and var_name in predictions and var_name in eval_targets:
            print(f"\nPlotting comprehensive comparison for {var_name} ...")
            
            # è·å–æ•°æ®
            input_data = eval_inputs[var_name].isel(time=-1, batch=0)  # è¾“å…¥æ•°æ®ï¼ˆæœ€åæ—¶åˆ»ï¼‰
            target_data = eval_targets[var_name].isel(time=0, batch=0)  # çœŸå®ç›®æ ‡å€¼ï¼ˆ6å°æ—¶åï¼‰
            gc_prediction = predictions[var_name].isel(time=0, batch=0)  # GraphCasté¢„æµ‹
            physics_prediction = physics_results[var_name]  # ç‰©ç†å…¬å¼é¢„æµ‹
            
            # è®¡ç®—å·®å¼‚ï¼ˆä½¿ç”¨çœŸå®ç›®æ ‡å€¼ï¼‰
            diff_target_gc = target_data - gc_prediction  # çœŸå®å€¼ - GraphCasté¢„æµ‹
            diff_target_physics = target_data - physics_prediction  # çœŸå®å€¼ - ç‰©ç†å…¬å¼é¢„æµ‹
            diff_gc_physics = gc_prediction - physics_prediction  # GraphCast - ç‰©ç†å…¬å¼
            
            # å¦‚æœæ˜¯3Dæ•°æ®ï¼Œé€‰æ‹©ä¸€ä¸ªå±‚æ¬¡è¿›è¡Œå¯è§†åŒ–
            if 'level' in input_data.dims:
                level_idx = len(input_data.level) // 2  # é€‰æ‹©ä¸­é—´å±‚
                input_data = input_data.isel(level=level_idx)
                target_data = target_data.isel(level=level_idx)
                gc_prediction = gc_prediction.isel(level=level_idx)
                physics_prediction = physics_prediction.isel(level=level_idx)
                diff_target_gc = diff_target_gc.isel(level=level_idx)
                diff_target_physics = diff_target_physics.isel(level=level_idx)
                diff_gc_physics = diff_gc_physics.isel(level=level_idx)
                level_info = f" (Level: {input_data.level.values:.0f} hPa)"
            else:
                level_info = ""
            
            # åˆ›å»º2x4çš„å­å›¾å¸ƒå±€ï¼ˆå¢åŠ ä¸€åˆ—æ˜¾ç¤ºçœŸå®ç›®æ ‡å€¼ï¼‰
            fig, axes = plt.subplots(2, 4, figsize=(26, 12))
            fig.suptitle(f'{var_name}{level_info} - Comprehensive Comparison ({physics_method_name})', fontsize=18, fontweight='bold')
            
            # ç¡®å®šé¢œè‰²èŒƒå›´
            vmin = min(input_data.min().values, target_data.min().values, 
                      gc_prediction.min().values, physics_prediction.min().values)
            vmax = max(input_data.max().values, target_data.max().values,
                      gc_prediction.max().values, physics_prediction.max().values)
            
            # 1. Input (Current Time)
            im1 = axes[0,0].imshow(input_data, cmap='RdYlBu_r', vmin=vmin, vmax=vmax,
                                  extent=[input_data.lon.min(), input_data.lon.max(), 
                                         input_data.lat.min(), input_data.lat.max()],
                                  origin='lower', aspect='auto')
            axes[0,0].set_title('Input (Current Time)', fontweight='bold', fontsize=14)
            axes[0,0].set_xlabel('Longitude')
            axes[0,0].set_ylabel('Latitude')
            plt.colorbar(im1, ax=axes[0,0], shrink=0.8)
            
            # 2. Target (Ground Truth, 6h Later)
            im2 = axes[0,1].imshow(target_data, cmap='RdYlBu_r', vmin=vmin, vmax=vmax,
                                  extent=[target_data.lon.min(), target_data.lon.max(), 
                                         target_data.lat.min(), target_data.lat.max()],
                                  origin='lower', aspect='auto')
            axes[0,1].set_title('Target (Ground Truth, 6h Later)', fontweight='bold', fontsize=14)
            axes[0,1].set_xlabel('Longitude')
            axes[0,1].set_ylabel('Latitude')
            plt.colorbar(im2, ax=axes[0,1], shrink=0.8)
            
            # 3. GraphCast Prediction
            im3 = axes[0,2].imshow(gc_prediction, cmap='RdYlBu_r', vmin=vmin, vmax=vmax,
                                  extent=[gc_prediction.lon.min(), gc_prediction.lon.max(), 
                                         gc_prediction.lat.min(), gc_prediction.lat.max()],
                                  origin='lower', aspect='auto')
            axes[0,2].set_title('GraphCast Prediction (6h Later)', fontweight='bold', fontsize=14)
            axes[0,2].set_xlabel('Longitude')
            axes[0,2].set_ylabel('Latitude')
            plt.colorbar(im3, ax=axes[0,2], shrink=0.8)
            
            # 4. Physics Prediction
            im4 = axes[0,3].imshow(physics_prediction, cmap='RdYlBu_r', vmin=vmin, vmax=vmax,
                                  extent=[physics_prediction.lon.min(), physics_prediction.lon.max(), 
                                         physics_prediction.lat.min(), physics_prediction.lat.max()],
                                  origin='lower', aspect='auto')
            axes[0,3].set_title(f'{physics_method_name} Prediction (6h Later)', fontweight='bold', fontsize=14)
            axes[0,3].set_xlabel('Longitude')
            axes[0,3].set_ylabel('Latitude')
            plt.colorbar(im4, ax=axes[0,3], shrink=0.8)
            
            # è®¡ç®—æ‰€æœ‰diffå›¾çš„ç»Ÿä¸€é¢œè‰²èŒƒå›´
            diff_max_unified = max(
                abs(diff_target_gc.min().values), abs(diff_target_gc.max().values),
                abs(diff_target_physics.min().values), abs(diff_target_physics.max().values),
                abs(diff_gc_physics.min().values), abs(diff_gc_physics.max().values)
            )
            
            # 5. Empty subplot
            axes[1,0].axis('off')
            
            # 6. Difference: Target - GraphCast
            im6 = axes[1,1].imshow(diff_target_gc, cmap='RdBu_r', vmin=-diff_max_unified, vmax=diff_max_unified,
                                  extent=[diff_target_gc.lon.min(), diff_target_gc.lon.max(), 
                                         diff_target_gc.lat.min(), diff_target_gc.lat.max()],
                                  origin='lower', aspect='auto')
            axes[1,1].set_title('Diff: Target - GraphCast', fontweight='bold', fontsize=14)
            axes[1,1].set_xlabel('Longitude')
            axes[1,1].set_ylabel('Latitude')
            plt.colorbar(im6, ax=axes[1,1], shrink=0.8)
            
            # 7. Difference: Target - Physics
            im7 = axes[1,2].imshow(diff_target_physics, cmap='RdBu_r', vmin=-diff_max_unified, vmax=diff_max_unified,
                                  extent=[diff_target_physics.lon.min(), diff_target_physics.lon.max(), 
                                         diff_target_physics.lat.min(), diff_target_physics.lat.max()],
                                  origin='lower', aspect='auto')
            axes[1,2].set_title(f'Diff: Target - {physics_method_name}', fontweight='bold', fontsize=14)
            axes[1,2].set_xlabel('Longitude')
            axes[1,2].set_ylabel('Latitude')
            plt.colorbar(im7, ax=axes[1,2], shrink=0.8)
            
            # 8. Difference: GraphCast - Physics
            im8 = axes[1,3].imshow(diff_gc_physics, cmap='RdBu_r', vmin=-diff_max_unified, vmax=diff_max_unified,
                                  extent=[diff_gc_physics.lon.min(), diff_gc_physics.lon.max(), 
                                         diff_gc_physics.lat.min(), diff_gc_physics.lat.max()],
                                  origin='lower', aspect='auto')
            axes[1,3].set_title(f'Diff: GraphCast - {physics_method_name}', fontweight='bold', fontsize=14)
            axes[1,3].set_xlabel('Longitude')
            axes[1,3].set_ylabel('Latitude')
            plt.colorbar(im8, ax=axes[1,3], shrink=0.8)
            
            # è®¡ç®—RMSEï¼ˆä½¿ç”¨çœŸå®ç›®æ ‡å€¼ï¼‰
            rmse_gc = float(np.sqrt(((diff_target_gc)**2).mean()))  # Target vs GraphCast
            rmse_physics = float(np.sqrt(((diff_target_physics)**2).mean()))  # Target vs Physics
            rmse_gc_physics = float(np.sqrt(((diff_gc_physics)**2).mean()))  # GraphCast vs Physics
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯åˆ°å„ä¸ªå·®å¼‚å›¾
            axes[1,1].text(0.02, 0.98, f'RMSE: {rmse_gc:.4f}', transform=axes[1,1].transAxes, 
                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            axes[1,2].text(0.02, 0.98, f'RMSE: {rmse_physics:.4f}', transform=axes[1,2].transAxes, 
                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            axes[1,3].text(0.02, 0.98, f'RMSE: {rmse_gc_physics:.4f}', transform=axes[1,3].transAxes, 
                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            plt.tight_layout()
            plt.show()
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print(f"\n{var_name}{level_info} Statistics:")
            print(f"  Input Range: [{input_data.min().values:.2f}, {input_data.max().values:.2f}]")
            print(f"  Target (Ground Truth) Range: [{target_data.min().values:.2f}, {target_data.max().values:.2f}]")
            print(f"  GraphCast Prediction Range: [{gc_prediction.min().values:.2f}, {gc_prediction.max().values:.2f}]")
            print(f"  {physics_method_name} Prediction Range: [{physics_prediction.min().values:.2f}, {physics_prediction.max().values:.2f}]")
            print(f"\n  Prediction Accuracy:")
            print(f"  RMSE (Target vs GraphCast): {rmse_gc:.4f}")
            print(f"  RMSE (Target vs {physics_method_name}): {rmse_physics:.4f}")
            print(f"  RMSE (GraphCast vs {physics_method_name}): {rmse_gc_physics:.4f}")
            
            # æ¯”è¾ƒå“ªä¸ªæ¨¡å‹æ›´å‡†ç¡®
            if rmse_gc < rmse_physics:
                print(f"  -> GraphCast is more accurate (lower RMSE by {rmse_physics - rmse_gc:.4f})")
            else:
                print(f"  -> {physics_method_name} is more accurate (lower RMSE by {rmse_gc - rmse_physics:.4f})")
    
    # ç»˜åˆ¶ç›¸å…³æ€§æ€»ç»“å›¾
    if correlations:
        print(f"\nPlotting correlation summary...")
        
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        var_names = list(correlations.keys())
        correlation_values = list(correlations.values())
        
        # æ ¹æ®ç›¸å…³æ€§å€¼è®¾ç½®é¢œè‰²
        colors_list = ['green' if c > 0.8 else 'orange' if c > 0.6 else 'red' for c in correlation_values]
        
        bars = ax.bar(var_names, correlation_values, color=colors_list, alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, correlation_values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax.set_ylabel('Correlation', fontweight='bold')
        ax.set_title(f'{physics_method_name} vs GraphCast Correlation', fontweight='bold')
        ax.set_ylim(0, 1)
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ å‚è€ƒçº¿
        ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Excellent (>0.8)')
        ax.axhline(y=0.6, color='orange', linestyle='--', alpha=0.5, label='Good (>0.6)')
        ax.axhline(y=0.4, color='red', linestyle='--', alpha=0.5, label='Fair (>0.4)')
        
        ax.legend()
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºå¹³å‡ç›¸å…³æ€§
        avg_correlation = np.mean(correlation_values)
        print(f"\nAverage Correlation: {avg_correlation:.4f}")
        
        if avg_correlation > 0.8:
            print(f"Overall: {physics_method_name} and GraphCast predictions are highly consistent!")
        elif avg_correlation > 0.6:
            print(f"Overall: {physics_method_name} and GraphCast predictions are fairly consistent.")
        elif avg_correlation > 0.4:
            print(f"Overall: There are some differences between {physics_method_name} and GraphCast predictions.")
        else:
            print(f"Overall: {physics_method_name} and GraphCast predictions differ significantly.")

except Exception as e:
    print(f"Error occurred during plotting: {e}")
    import traceback
    traceback.print_exc()


# %%
